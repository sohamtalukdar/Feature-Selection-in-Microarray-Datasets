{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"350945c4-28f1-4f6f-b58b-010a2e0e7296","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["import time\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from tabulate import tabulate\n","from datetime import timedelta\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9e00a4c3-0de8-4d68-8859-c7753e418dcd","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import UnivariateFeatureSelector\n","from pyspark.ml.feature import ChiSqSelector, VectorAssembler\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.classification import DecisionTreeClassifier as pyspark_DecisionTreeClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cb6d14ac-6bd3-4486-9779-065bb8ccdd5a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Creating a new spark session\n","spark = SparkSession.builder.master(\"local[*]\").appName(\"MLlib lab\").getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"028d3650-724e-45ad-82dd-bc99a8cb43c9","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# load in preprocessed data\n","sparkDF = spark.read.option(\"maxColumns\", 22285).parquet(\"/mnt/the-data-transformers/spark_output/preprocessed_data_final2.pqt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"22518f2e-6057-4d90-96e4-3e3e4b202750","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["'''\n","Function: spark_UnivariateFeatureSelector\n","INPUT:\n","------\n","i: Number of Features to select\n","_vector_sparkDF: DataFrame from which the Features will be selected\n","\n","OUTPUT:\n","-------\n","1. The model's list of feature importances\n","'''\n","def spark_UnivariateFeatureSelector(i, _vector_sparkDF):\n","  # Selecting the best features from the entire dataset/chunk passed in\n","  selector = UnivariateFeatureSelector(\n","    featuresCol=\"features\", \n","    outputCol=\"selectedFeatures\", \n","    labelCol=\"type\", \n","    selectionMode=\"numTopFeatures\"\n","  )\n","  selector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(i)\n","  result = selector.fit(_vector_sparkDF).transform(_vector_sparkDF)\n","\n","  # Using pyspark DecisionTreeClassifier to define and fit the ML model\n","  dt = pyspark_DecisionTreeClassifier(labelCol=\"type\", featuresCol=\"selectedFeatures\")\n","  model = dt.fit(result)\n","  \n","  return model.featureImportances \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1b2de0d9-1a65-47db-ab2b-453a12decaf1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["'''\n","Function: obtain_accuracy\n","INPUT:\n","------\n","sparkDF: the spark DataFrame consisting of the dataset\n","\n","OUTPUT:\n","-------\n","1. The accuracy returned by the evaluator\n","'''\n","def obtain_accuracy(sparkDF):\n","  # Make predictions\n","  selector = UnivariateFeatureSelector(\n","    featuresCol=\"features\", \n","    outputCol=\"selectedFeatures\", \n","    labelCol=\"type\", \n","    selectionMode=\"numTopFeatures\"\n","  )\n","\n","  vecAssembler = VectorAssembler(inputCols=sparkDF.columns, outputCol=\"features\")\n","  sparkDF = vecAssembler.transform(sparkDF)\n","  \n","  selector.setFeatureType(\"continuous\").setLabelType(\"categorical\") #.setSelectionThreshold(i)\n","  result = selector.fit(sparkDF).transform(sparkDF)\n","\n","  # Splitting the data into training & testing\n","  (train, test) = result.randomSplit([0.8, 0.2])\n","\n","  # Using pyspark DecisionTreeClassifier to define and fit the ML model\n","  dt = pyspark_DecisionTreeClassifier(labelCol=\"type\", featuresCol=\"selectedFeatures\")\n","  model = dt.fit(train)\n","  predictions = model.transform(test)\n","\n","  # Evaluating the predictions\n","  evaluator = MulticlassClassificationEvaluator(\n","    labelCol=\"type\", \n","    predictionCol=\"prediction\", \n","    metricName=\"accuracy\"\n","  )\n","\n","  accuracy = evaluator.evaluate(predictions)\n","  return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"57dfa612-381f-4463-8073-0526c0a5f2e0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["'''\n","Function: get_important_features\n","INPUT:\n","------\n","num: the number of features present in a chunk/subset\n","i: the chunk number\n","\n","OUTPUT:\n","-------\n","1. The features selected by the UnivariateFeatureSelector\n","'''\n","def get_important_features(num, i):\n","  num_columns = num\n","  num_features = num\n","  \n","  # get chunk using indexing\n","  print(\"range, lower: \"+str(1+(i*num_columns))+ \" upper:\"+str(num_columns*(i+1)))\n","  chunk =  sparkDF.select(*sparkDF.columns[1+(i*num_columns):num_columns*(i+1)], sparkDF['type']).distinct()\n","  vecAssembler = VectorAssembler(inputCols=chunk.columns, outputCol=\"features\")\n","  newDF = vecAssembler.transform(chunk)\n","  selected_features = spark_UnivariateFeatureSelector(num_features, newDF)\n","\n","  return selected_features\n","\n","'''\n","Function: remove_features\n","INPUT:\n","------\n","sparkDF: the spark DataFrame consisting of the dataset\n","\n","OUTPUT:\n","-------\n","1. The new spark DataFrame that only contains the most important subset of features\n","'''\n","def remove_features(sparkDF, columns_input, chunks_input):\n","  print(\"removing features\")\n","  columns = columns_input # 2000\n","  chunks = chunks_input # 11\n","  toDrop = []\n","  \n","  # loop through chunks\n","  for i in range(0, chunks):\n","    print(\"chunk: \"+str(i+1))\n","\n","    # set num columns to amount left if below set columns\n","    if (len(sparkDF.columns) < columns):\n","      print(\"updating remaining columns\")\n","      columns = len(sparkDF.columns)\n","      \n","    # get important features\n","    important_feature_vectors = get_important_features(columns, i) \n","    vector = important_feature_vectors.toArray()\n","    index  = columns*i\n","    \n","    # loop through all colums and and add low importance ones to array\n","    for c in range(0, columns):\n","      if (vector[c] == 0): # if unimportant\n","        tmp_index = index + c\n","        if (sparkDF.columns[tmp_index] and sparkDF.columns[tmp_index] != \"type\"): # if index is correct\n","          toDrop.append(sparkDF.columns[tmp_index])\n","        \n","  # drop low importance columns\n","  sparkDF = sparkDF.drop(*toDrop)\n","  print(\"remaining columns: \"+str(len(sparkDF.columns)))\n","\n","  return sparkDF"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cb29da74-26b2-45e0-97a3-fe377beed0af","showTitle":false,"title":""}},"source":["Local Approach"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3f4426b9-c67c-4467-9223-60570ce954e2","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">removing features\n","chunk: 1\n","range, lower: 1 upper:2000\n","chunk: 2\n","range, lower: 2001 upper:4000\n","chunk: 3\n","range, lower: 4001 upper:6000\n","chunk: 4\n","range, lower: 6001 upper:8000\n","chunk: 5\n","range, lower: 8001 upper:10000\n","chunk: 6\n","range, lower: 10001 upper:12000\n","chunk: 7\n","range, lower: 12001 upper:14000\n","chunk: 8\n","range, lower: 14001 upper:16000\n","chunk: 9\n","range, lower: 16001 upper:18000\n","chunk: 10\n","range, lower: 18001 upper:20000\n","chunk: 11\n","range, lower: 20001 upper:22000\n","remaining columns: 327\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">removing features\nchunk: 1\nrange, lower: 1 upper:2000\nchunk: 2\nrange, lower: 2001 upper:4000\nchunk: 3\nrange, lower: 4001 upper:6000\nchunk: 4\nrange, lower: 6001 upper:8000\nchunk: 5\nrange, lower: 8001 upper:10000\nchunk: 6\nrange, lower: 10001 upper:12000\nchunk: 7\nrange, lower: 12001 upper:14000\nchunk: 8\nrange, lower: 14001 upper:16000\nchunk: 9\nrange, lower: 16001 upper:18000\nchunk: 10\nrange, lower: 18001 upper:20000\nchunk: 11\nrange, lower: 20001 upper:22000\nremaining columns: 327\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# local approach - running univariate selector on every chunk of the dataset whilst aggregating the results\n","sparkDF = remove_features(sparkDF, 2000, 11)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cfbd0b1e-a92a-4a94-9f25-edf5bed27cb0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[10]: 327</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[10]: 327</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# count number of features selected\n","len(sparkDF.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9706cbad-0d4a-40f0-a31a-495de17c50bb","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# check accuracy of the local solution's model\n","local_solution_accuracy = obtain_accuracy(sparkDF)\n","local_solution_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4d0ed07e-fd1a-4e19-be09-2b6814573c1c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[19]: 1.0</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[19]: 1.0</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["local_solution_accuracy"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9002d659-8401-4ba5-a95f-77d157d859c9","showTitle":false,"title":""}},"source":["Global Approach"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7cf43ea1-3167-43ae-91ca-3d9418094095","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# reset sparkDF to contain the initial preprocessed data again to run for the global solution\n","sparkDF = spark.read.option(\"maxColumns\", 22285).parquet(\"/mnt/the-data-transformers/spark_output/preprocessed_data_final2.pqt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"07d02cc5-ad49-43d3-a269-4f38625dacea","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">removing features\n","chunk: 1\n","range, lower: 1 upper:22000\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">removing features\nchunk: 1\nrange, lower: 1 upper:22000\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n","<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n","<span class=\"ansi-green-fg\">&lt;command-3841274923039632&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># global approach - running univariate selector on 1 chunk containing the entire dataset</span>\n","<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>sparkDF <span class=\"ansi-blue-fg\">=</span> remove_features<span class=\"ansi-blue-fg\">(</span>sparkDF<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">22000</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n","\n","<span class=\"ansi-green-fg\">&lt;command-3841274923039619&gt;</span> in <span class=\"ansi-cyan-fg\">remove_features</span><span class=\"ansi-blue-fg\">(sparkDF, columns_input, chunks_input)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     30</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>     <span class=\"ansi-red-fg\"># get important features</span>\n","<span class=\"ansi-green-fg\">---&gt; 32</span><span class=\"ansi-red-fg\">     </span>important_feature_vectors <span class=\"ansi-blue-fg\">=</span> get_important_features<span class=\"ansi-blue-fg\">(</span>columns<span class=\"ansi-blue-fg\">,</span> i<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     33</span>     vector <span class=\"ansi-blue-fg\">=</span> important_feature_vectors<span class=\"ansi-blue-fg\">.</span>toArray<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     34</span>     index  <span class=\"ansi-blue-fg\">=</span> columns<span class=\"ansi-blue-fg\">*</span>i\n","\n","<span class=\"ansi-green-fg\">&lt;command-3841274923039619&gt;</span> in <span class=\"ansi-cyan-fg\">get_important_features</span><span class=\"ansi-blue-fg\">(num, i)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">      9</span>   vecAssembler <span class=\"ansi-blue-fg\">=</span> VectorAssembler<span class=\"ansi-blue-fg\">(</span>inputCols<span class=\"ansi-blue-fg\">=</span>chunk<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">,</span> outputCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;features&#34;</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     10</span>   newDF <span class=\"ansi-blue-fg\">=</span> vecAssembler<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>chunk<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-fg\">---&gt; 11</span><span class=\"ansi-red-fg\">   </span>selected_features <span class=\"ansi-blue-fg\">=</span> spark_UnivariateFeatureSelector<span class=\"ansi-blue-fg\">(</span>num_features<span class=\"ansi-blue-fg\">,</span> newDF<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">     13</span>   <span class=\"ansi-green-fg\">return</span> selected_features\n","\n","<span class=\"ansi-green-fg\">&lt;command-3841274923039618&gt;</span> in <span class=\"ansi-cyan-fg\">spark_UnivariateFeatureSelector</span><span class=\"ansi-blue-fg\">(i, _vector_sparkDF)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     26</span>   <span class=\"ansi-red-fg\"># Using pyspark DecisionTreeClassifier to define and fit the ML model</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     27</span>   dt <span class=\"ansi-blue-fg\">=</span> pyspark_DecisionTreeClassifier<span class=\"ansi-blue-fg\">(</span>labelCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;type&#34;</span><span class=\"ansi-blue-fg\">,</span> featuresCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;selectedFeatures&#34;</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-fg\">---&gt; 28</span><span class=\"ansi-red-fg\">   </span>model <span class=\"ansi-blue-fg\">=</span> dt<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>result<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     29</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">     30</span>   <span class=\"ansi-green-fg\">return</span> model<span class=\"ansi-blue-fg\">.</span>featureImportances\n","\n","<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise TypeError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-fg\">--&gt; 335</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    337</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>         &#34;&#34;&#34;\n","<span class=\"ansi-green-intense-fg ansi-bold\">    331</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-fg\">--&gt; 332</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n","</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n","</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n","\n","<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o72538.fit.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 1025.0 failed 4 times, most recent failure: Lost task 12.3 in stage 1025.0 (TID 3706) (10.139.64.7 executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2973)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2920)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2914)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2914)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3182)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3123)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3111)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3841274923039632&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># global approach - running univariate selector on 1 chunk containing the entire dataset</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>sparkDF <span class=\"ansi-blue-fg\">=</span> remove_features<span class=\"ansi-blue-fg\">(</span>sparkDF<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">22000</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3841274923039619&gt;</span> in <span class=\"ansi-cyan-fg\">remove_features</span><span class=\"ansi-blue-fg\">(sparkDF, columns_input, chunks_input)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     30</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>     <span class=\"ansi-red-fg\"># get important features</span>\n<span class=\"ansi-green-fg\">---&gt; 32</span><span class=\"ansi-red-fg\">     </span>important_feature_vectors <span class=\"ansi-blue-fg\">=</span> get_important_features<span class=\"ansi-blue-fg\">(</span>columns<span class=\"ansi-blue-fg\">,</span> i<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     33</span>     vector <span class=\"ansi-blue-fg\">=</span> important_feature_vectors<span class=\"ansi-blue-fg\">.</span>toArray<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     34</span>     index  <span class=\"ansi-blue-fg\">=</span> columns<span class=\"ansi-blue-fg\">*</span>i\n\n<span class=\"ansi-green-fg\">&lt;command-3841274923039619&gt;</span> in <span class=\"ansi-cyan-fg\">get_important_features</span><span class=\"ansi-blue-fg\">(num, i)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span>   vecAssembler <span class=\"ansi-blue-fg\">=</span> VectorAssembler<span class=\"ansi-blue-fg\">(</span>inputCols<span class=\"ansi-blue-fg\">=</span>chunk<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">,</span> outputCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;features&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span>   newDF <span class=\"ansi-blue-fg\">=</span> vecAssembler<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>chunk<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 11</span><span class=\"ansi-red-fg\">   </span>selected_features <span class=\"ansi-blue-fg\">=</span> spark_UnivariateFeatureSelector<span class=\"ansi-blue-fg\">(</span>num_features<span class=\"ansi-blue-fg\">,</span> newDF<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     13</span>   <span class=\"ansi-green-fg\">return</span> selected_features\n\n<span class=\"ansi-green-fg\">&lt;command-3841274923039618&gt;</span> in <span class=\"ansi-cyan-fg\">spark_UnivariateFeatureSelector</span><span class=\"ansi-blue-fg\">(i, _vector_sparkDF)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     26</span>   <span class=\"ansi-red-fg\"># Using pyspark DecisionTreeClassifier to define and fit the ML model</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     27</span>   dt <span class=\"ansi-blue-fg\">=</span> pyspark_DecisionTreeClassifier<span class=\"ansi-blue-fg\">(</span>labelCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;type&#34;</span><span class=\"ansi-blue-fg\">,</span> featuresCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;selectedFeatures&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 28</span><span class=\"ansi-red-fg\">   </span>model <span class=\"ansi-blue-fg\">=</span> dt<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>result<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     30</span>   <span class=\"ansi-green-fg\">return</span> model<span class=\"ansi-blue-fg\">.</span>featureImportances\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise TypeError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 335</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    337</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    331</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 332</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o72538.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 1025.0 failed 4 times, most recent failure: Lost task 12.3 in stage 1025.0 (TID 3706) (10.139.64.7 executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2920)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2914)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2914)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3123)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3111)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 1025.0 failed 4 times, most recent failure: Lost task 12.3 in stage 1025.0 (TID 3706) (10.139.64.7 executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["# global approach - running univariate selector on 1 chunk containing the entire dataset (no aggregation required)\n","sparkDF = remove_features(sparkDF, 22000, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"672839ee-af0f-4387-8b66-68b07cbd1609","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[18]: 327</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[18]: 327</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# count number of features selected\n","len(sparkDF.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"732bb693-1fcb-4903-96b2-61b287b00d87","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Cancelled","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["# check accuracy of the global solution's model\n","global_solution_accuracy = obtain_accuracy(sparkDF)\n","global_solution_accuracy"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"4_Big-Data-Solution_Leukemia","notebookOrigID":3841274923039613,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
